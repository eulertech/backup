{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a trained LSTM to predict Brent price\n",
    "Author: Valentin Todorov\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Import the functions and classes we'll need\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "### Provide input values to parameters\n",
    "# Read in the data\n",
    "cwd = os.getcwd()\n",
    "dataLocation = \"/home/valentint/EAA_Analytics/Personal/VT/\"\n",
    "inputFile = \"file_for_lstm_model_logdiff\"\n",
    "\n",
    "brentPriceDf = pd.read_csv(dataLocation + inputFile + \".csv\")\n",
    "brentPriceDf.head(10)\n",
    "\n",
    "# Remove the forecasts from the data\n",
    "# inputData.BrentPrice[[inputData.Date > 2017-01-01 00:00:00]] = 0\n",
    "\n",
    "# Format \"Date\" field as date\n",
    "brentPriceDf[['date']] = pd.to_datetime(brentPriceDf.date)\n",
    "\n",
    "# Convert the data frame to a Numpy array\n",
    "brentPriceArr = brentPriceDf.iloc[:, 0:].values\n",
    "\n",
    "# Select random seed\n",
    "randSeed = 7896\n",
    "\n",
    "# Provide names of input features\n",
    "inputDataFrame = brentPriceDf\n",
    "inputData = brentPriceArr\n",
    "\n",
    "# Create a Numpy array from the input data\n",
    "dataframe = inputData[:, 1:, ]\n",
    "# xVarColumns = [1, 2, 3]                                   # Select features: BY DEFAULT, it uses all features in columns 1:END for predictors\n",
    "yVarColumns = [0]                                           # Select target: The target should always be in the first column\n",
    "number_of_features = len(list(brentPriceDf)) - 2            # Calculate the number of features to be used in the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 \n",
      "The analytical array shape is (includes the target): (207, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>v137013095</th>\n",
       "      <th>v155478663</th>\n",
       "      <th>v179715733</th>\n",
       "      <th>pmiMfctg</th>\n",
       "      <th>inventoryChange</th>\n",
       "      <th>globalInventory</th>\n",
       "      <th>v1106237782</th>\n",
       "      <th>v1106492632</th>\n",
       "      <th>...</th>\n",
       "      <th>ipOECD</th>\n",
       "      <th>usDollar</th>\n",
       "      <th>globalDemand</th>\n",
       "      <th>gasolineDemand</th>\n",
       "      <th>oecdLiquidsDemand</th>\n",
       "      <th>vehicleSales</th>\n",
       "      <th>oecdStocks</th>\n",
       "      <th>opecSupply</th>\n",
       "      <th>auto_sales</th>\n",
       "      <th>v134253300_level_lag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>-0.005808</td>\n",
       "      <td>0.413467</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.598205</td>\n",
       "      <td>4.007333</td>\n",
       "      <td>-2292</td>\n",
       "      <td>-2639</td>\n",
       "      <td>0.009391</td>\n",
       "      <td>-0.017291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.017242</td>\n",
       "      <td>0.044344</td>\n",
       "      <td>0.062864</td>\n",
       "      <td>0.040506</td>\n",
       "      <td>0.070056</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>0.026415</td>\n",
       "      <td>0.070056</td>\n",
       "      <td>27.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>-0.197803</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693756</td>\n",
       "      <td>4.025352</td>\n",
       "      <td>-761</td>\n",
       "      <td>-1102</td>\n",
       "      <td>-0.017831</td>\n",
       "      <td>-0.104069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.018575</td>\n",
       "      <td>-0.001762</td>\n",
       "      <td>-0.021877</td>\n",
       "      <td>0.258887</td>\n",
       "      <td>-0.004082</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.258887</td>\n",
       "      <td>27.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-05-01</td>\n",
       "      <td>0.195252</td>\n",
       "      <td>-0.058480</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.025352</td>\n",
       "      <td>2321</td>\n",
       "      <td>2984</td>\n",
       "      <td>-0.039785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.045370</td>\n",
       "      <td>-0.007474</td>\n",
       "      <td>-0.042445</td>\n",
       "      <td>-0.234349</td>\n",
       "      <td>0.010184</td>\n",
       "      <td>0.028647</td>\n",
       "      <td>-0.234349</td>\n",
       "      <td>22.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>0.079930</td>\n",
       "      <td>0.175541</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.098425</td>\n",
       "      <td>4.007333</td>\n",
       "      <td>1190</td>\n",
       "      <td>2137</td>\n",
       "      <td>0.042926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.025649</td>\n",
       "      <td>0.087765</td>\n",
       "      <td>0.006379</td>\n",
       "      <td>0.016841</td>\n",
       "      <td>0.087765</td>\n",
       "      <td>27.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>-0.040219</td>\n",
       "      <td>0.584112</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.393314</td>\n",
       "      <td>3.988984</td>\n",
       "      <td>274</td>\n",
       "      <td>998</td>\n",
       "      <td>0.042772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.008368</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>0.008083</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>-0.013884</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>29.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-08-01</td>\n",
       "      <td>0.047269</td>\n",
       "      <td>0.290360</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.195886</td>\n",
       "      <td>3.988984</td>\n",
       "      <td>1220</td>\n",
       "      <td>2167</td>\n",
       "      <td>-0.007182</td>\n",
       "      <td>-0.019545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.006437</td>\n",
       "      <td>-0.005509</td>\n",
       "      <td>-0.012144</td>\n",
       "      <td>-0.059429</td>\n",
       "      <td>0.015885</td>\n",
       "      <td>0.011771</td>\n",
       "      <td>-0.059429</td>\n",
       "      <td>28.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-09-01</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.970292</td>\n",
       "      <td>-896</td>\n",
       "      <td>72</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>-0.082238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.034214</td>\n",
       "      <td>0.028078</td>\n",
       "      <td>0.047035</td>\n",
       "      <td>-0.103427</td>\n",
       "      <td>-0.009144</td>\n",
       "      <td>0.020687</td>\n",
       "      <td>-0.103427</td>\n",
       "      <td>29.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-10-01</td>\n",
       "      <td>-0.053199</td>\n",
       "      <td>0.521135</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.488759</td>\n",
       "      <td>3.951244</td>\n",
       "      <td>222</td>\n",
       "      <td>952</td>\n",
       "      <td>0.031515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008299</td>\n",
       "      <td>-0.011820</td>\n",
       "      <td>-0.037840</td>\n",
       "      <td>-0.015515</td>\n",
       "      <td>0.115226</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.115226</td>\n",
       "      <td>32.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>0.050129</td>\n",
       "      <td>0.172811</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.097276</td>\n",
       "      <td>3.931826</td>\n",
       "      <td>1707</td>\n",
       "      <td>2520</td>\n",
       "      <td>-0.030788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016394</td>\n",
       "      <td>-0.010204</td>\n",
       "      <td>-0.018148</td>\n",
       "      <td>-0.008442</td>\n",
       "      <td>-0.082335</td>\n",
       "      <td>-0.002417</td>\n",
       "      <td>0.019062</td>\n",
       "      <td>-0.082335</td>\n",
       "      <td>30.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-12-01</td>\n",
       "      <td>-0.251842</td>\n",
       "      <td>0.172513</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.097182</td>\n",
       "      <td>3.931826</td>\n",
       "      <td>2356</td>\n",
       "      <td>2583</td>\n",
       "      <td>-0.021526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.010087</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>-0.016540</td>\n",
       "      <td>0.004932</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>-0.016540</td>\n",
       "      <td>32.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    target  v137013095  v155478663  v179715733  pmiMfctg  \\\n",
       "0 2000-03-01 -0.005808    0.413467         3.5    0.598205  4.007333   \n",
       "1 2000-04-01 -0.197803    0.588235         1.0    0.693756  4.025352   \n",
       "2 2000-05-01  0.195252   -0.058480        -5.1    0.000000  4.025352   \n",
       "3 2000-06-01  0.079930    0.175541         8.1    0.098425  4.007333   \n",
       "4 2000-07-01 -0.040219    0.584112         1.0    0.393314  3.988984   \n",
       "5 2000-08-01  0.047269    0.290360        -0.8    0.195886  3.988984   \n",
       "6 2000-09-01  0.087402    0.000000         2.4    0.000000  3.970292   \n",
       "7 2000-10-01 -0.053199    0.521135         5.6    0.488759  3.951244   \n",
       "8 2000-11-01  0.050129    0.172811        -0.4    0.097276  3.931826   \n",
       "9 2000-12-01 -0.251842    0.172513         0.7   -0.097182  3.931826   \n",
       "\n",
       "   inventoryChange  globalInventory  v1106237782  v1106492632  \\\n",
       "0            -2292            -2639     0.009391    -0.017291   \n",
       "1             -761            -1102    -0.017831    -0.104069   \n",
       "2             2321             2984    -0.039785     0.000000   \n",
       "3             1190             2137     0.042926     0.000000   \n",
       "4              274              998     0.042772     0.000000   \n",
       "5             1220             2167    -0.007182    -0.019545   \n",
       "6             -896               72     0.031052    -0.082238   \n",
       "7              222              952     0.031515     0.000000   \n",
       "8             1707             2520    -0.030788     0.000000   \n",
       "9             2356             2583    -0.021526     0.000000   \n",
       "\n",
       "           ...             ipOECD  usDollar  globalDemand  gasolineDemand  \\\n",
       "0          ...           0.010152  0.017242      0.044344        0.062864   \n",
       "1          ...           0.000000  0.000000     -0.018575       -0.001762   \n",
       "2          ...           0.010050  0.000000     -0.045370       -0.007474   \n",
       "3          ...           0.000000  0.025318      0.017391        0.033200   \n",
       "4          ...           0.000000 -0.008368      0.012034        0.008083   \n",
       "5          ...           0.009950  0.000000     -0.006437       -0.005509   \n",
       "6          ...           0.000000  0.008368      0.034214        0.028078   \n",
       "7          ...           0.000000  0.008299     -0.011820       -0.037840   \n",
       "8          ...           0.000000  0.016394     -0.010204       -0.018148   \n",
       "9          ...           0.000000  0.008097      0.010087        0.007075   \n",
       "\n",
       "   oecdLiquidsDemand  vehicleSales  oecdStocks  opecSupply  auto_sales  \\\n",
       "0           0.040506      0.070056   -0.006329    0.026415    0.070056   \n",
       "1          -0.021877      0.258887   -0.004082    0.000804    0.258887   \n",
       "2          -0.042445     -0.234349    0.010184    0.028647   -0.234349   \n",
       "3           0.025649      0.087765    0.006379    0.016841    0.087765   \n",
       "4           0.005002      0.000921    0.008034   -0.013884    0.000921   \n",
       "5          -0.012144     -0.059429    0.015885    0.011771   -0.059429   \n",
       "6           0.047035     -0.103427   -0.009144    0.020687   -0.103427   \n",
       "7          -0.015515      0.115226    0.002762    0.000283    0.115226   \n",
       "8          -0.008442     -0.082335   -0.002417    0.019062   -0.082335   \n",
       "9           0.001882     -0.016540    0.004932    0.008740   -0.016540   \n",
       "\n",
       "   v134253300_level_lag  \n",
       "0                 27.63  \n",
       "1                 27.47  \n",
       "2                 22.54  \n",
       "3                 27.40  \n",
       "4                 29.68  \n",
       "5                 28.51  \n",
       "6                 29.89  \n",
       "7                 32.62  \n",
       "8                 30.93  \n",
       "9                 32.52  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################\n",
    "# Sanity checks of the data\n",
    "########################################################\n",
    "\n",
    "print(number_of_features, \"\\nThe analytical array shape is (includes the target):\", dataframe.shape)\n",
    "brentPriceDf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Transform the data\n",
    "########################################################\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(randSeed)\n",
    "\n",
    "## NO NEED FOR ARRAYS - Extract the NumPy array from the dataframe and convert the integer values to\n",
    "# floating point values, which are more suitable for modeling with a neural network\n",
    "# dataset = dataframe.values\n",
    "# dataset = dataset.astype('float32')\n",
    "\n",
    "# Normalize the data between 0 - 1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1), copy = True)\n",
    "dataset = scaler.fit_transform(dataframe)\n",
    "\n",
    "# Split the data in train and validaiton\n",
    "trainSize = 166                         # This is the number of records used in the training set (03/2000 - 12/2013)\n",
    "train = dataset[0:trainSize, :]\n",
    "validate = dataset[trainSize:len(dataset), :]\n",
    "\n",
    "## Modify the data for the LSTM network - The LSTM network expects the input data (X)\n",
    "# to be provided with a specific array structure in the form of: [samples, time steps, features].\n",
    "trainX = train[:, 1:]\n",
    "validateX = validate[:, 1:]\n",
    "\n",
    "trainY = train[:, yVarColumns]\n",
    "validateY = validate[:, yVarColumns]\n",
    "\n",
    "dataframe_length = len(trainY)\n",
    "# dataframe_dim = Need to figure out how to count the columns of the array\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = trainX.reshape(trainX.shape[0], 1, trainX.shape[1])\n",
    "validateX = validateX.reshape(validateX.shape[0], 1, validateX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 41 207\n",
      "(166, 1, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.69391165,  0.74943948,  0.54396334,  0.61831711,  0.74812946,\n",
       "        0.65479768,  0.56254363,  0.72812015,  0.61744967,  0.61735495,\n",
       "        0.63549953,  0.74451553,  0.63491787,  0.58059607,  0.61667019,\n",
       "        0.72464717,  0.63422393,  0.50890441,  0.56254363,  0.68791345,\n",
       "        0.47334572,  0.54465382,  0.54464374,  0.61627355,  0.61618284,\n",
       "        0.65179165,  0.70494047,  0.597984  ,  0.58024407,  0.63330598,\n",
       "        0.6508    ,  0.61535077,  0.63283631,  0.61514676,  0.61505982,\n",
       "        0.7023557 ,  0.73654305,  0.61445914,  0.44160504,  0.51051476,\n",
       "        0.59728643,  0.66665819,  0.70090963,  0.66586816,  0.52821376,\n",
       "        0.57972713,  0.64841469,  0.699567  ,  0.63076111,  0.63061496,\n",
       "        0.61348798,  0.69817776,  0.68071901,  0.5961829 ,  0.57934548,\n",
       "        0.66330143,  0.72994244,  0.71241294,  0.56254363,  0.54596966,\n",
       "        0.69520456,  0.67813929,  0.66126645,  0.54614079,  0.57895494,\n",
       "        0.75937767,  0.75816576,  1.        ,  0.61048975,  0.40296405,\n",
       "        0.56254363,  0.75500578,  0.57848557,  0.61034548,  0.72164375,\n",
       "        0.65752807,  0.6414614 ,  0.73573255,  0.70347544,  0.40664425,\n",
       "        0.42154232,  0.57828028,  0.73556107,  0.6152628 ,  0.68576772,\n",
       "        0.72776342,  0.65788156,  0.69385833,  0.63615215,  0.61911724,\n",
       "        0.57233841,  0.69718042,  0.66050517,  0.81223841,  0.6546202 ,\n",
       "        0.67208513,  0.63936354,  0.67622578,  0.63607682,  0.75055591,\n",
       "        0.89546119,  0.78944371,  0.51525134,  0.58971185,  0.28935157,\n",
       "        0.        ,  0.30094573,  0.64295204,  0.67827931,  0.53117539,\n",
       "        0.59454095,  0.60929637,  0.82624135,  0.55307658,  0.66893167,\n",
       "        0.62389235,  0.65792189,  0.66893603,  0.57907083,  0.58315534,\n",
       "        0.5323035 ,  0.57307195,  0.56985254,  0.54602928,  0.54923755,\n",
       "        0.6219341 ,  0.6089745 ,  0.61386381,  0.67316987,  0.64304959,\n",
       "        0.69015947,  0.66558252,  0.66467486,  0.72691912,  0.71168592,\n",
       "        0.663634  ,  0.56254363,  0.64578821,  0.66276813,  0.63153869,\n",
       "        0.58399652,  0.62125406,  0.57009617,  0.64909759,  0.6304552 ,\n",
       "        0.62905799,  0.61531068,  0.4968328 ,  0.53628817,  0.57171978,\n",
       "        0.74712566,  0.7141377 ,  0.64822698,  0.509187  ,  0.55869659,\n",
       "        0.6162713 ,  0.75089035,  0.46899343,  0.4936099 ,  0.57912918,\n",
       "        0.62528924,  0.63295913,  0.62175285,  0.60462109,  0.57423902,\n",
       "        0.6173286 ])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################\n",
    "# Sanity checks of the data\n",
    "########################################################\n",
    "\n",
    "print(len(train), len(validate), len(dataset))\n",
    "print(trainX.shape)\n",
    "\n",
    "train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 10)                1360      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,371\n",
      "Trainable params: 1,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 166 samples, validate on 41 samples\n",
      "Epoch 1/50\n",
      "166/166 [==============================] - 0s - loss: 0.3949 - acc: 0.0060 - val_loss: 0.2983 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "166/166 [==============================] - 0s - loss: 0.3270 - acc: 0.0060 - val_loss: 0.2488 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "166/166 [==============================] - 0s - loss: 0.2785 - acc: 0.0060 - val_loss: 0.2044 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "166/166 [==============================] - 0s - loss: 0.2380 - acc: 0.0060 - val_loss: 0.1659 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "166/166 [==============================] - 0s - loss: 0.1834 - acc: 0.0060 - val_loss: 0.1331 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "166/166 [==============================] - 0s - loss: 0.1509 - acc: 0.0060 - val_loss: 0.1055 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "166/166 [==============================] - 0s - loss: 0.1187 - acc: 0.0060 - val_loss: 0.0826 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "166/166 [==============================] - 0s - loss: 0.0940 - acc: 0.0060 - val_loss: 0.0647 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "166/166 [==============================] - 0s - loss: 0.0751 - acc: 0.0060 - val_loss: 0.0519 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0638 - acc: 0.0000e+0 - 0s - loss: 0.0615 - acc: 0.0060 - val_loss: 0.0436 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "166/166 [==============================] - 0s - loss: 0.0454 - acc: 0.0060 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "166/166 [==============================] - 0s - loss: 0.0346 - acc: 0.0120 - val_loss: 0.0376 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "166/166 [==============================] - 0s - loss: 0.0325 - acc: 0.0120 - val_loss: 0.0383 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "166/166 [==============================] - 0s - loss: 0.0293 - acc: 0.0120 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "166/166 [==============================] - 0s - loss: 0.0283 - acc: 0.0120 - val_loss: 0.0422 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "166/166 [==============================] - 0s - loss: 0.0325 - acc: 0.0120 - val_loss: 0.0439 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "166/166 [==============================] - 0s - loss: 0.0300 - acc: 0.0120 - val_loss: 0.0447 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "166/166 [==============================] - 0s - loss: 0.0338 - acc: 0.0120 - val_loss: 0.0447 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "166/166 [==============================] - 0s - loss: 0.0322 - acc: 0.0120 - val_loss: 0.0441 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "166/166 [==============================] - 0s - loss: 0.0294 - acc: 0.0120 - val_loss: 0.0431 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "166/166 [==============================] - 0s - loss: 0.0309 - acc: 0.0120 - val_loss: 0.0422 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "166/166 [==============================] - 0s - loss: 0.0303 - acc: 0.0120 - val_loss: 0.0414 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "166/166 [==============================] - 0s - loss: 0.0292 - acc: 0.0120 - val_loss: 0.0407 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "166/166 [==============================] - 0s - loss: 0.0292 - acc: 0.0120 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "166/166 [==============================] - 0s - loss: 0.0275 - acc: 0.0120 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "166/166 [==============================] - 0s - loss: 0.0300 - acc: 0.0120 - val_loss: 0.0395 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "166/166 [==============================] - 0s - loss: 0.0305 - acc: 0.0120 - val_loss: 0.0392 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "166/166 [==============================] - 0s - loss: 0.0288 - acc: 0.0120 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "166/166 [==============================] - 0s - loss: 0.0303 - acc: 0.0060 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "166/166 [==============================] - 0s - loss: 0.0289 - acc: 0.0120 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "166/166 [==============================] - 0s - loss: 0.0299 - acc: 0.0120 - val_loss: 0.0393 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "166/166 [==============================] - 0s - loss: 0.0321 - acc: 0.0120 - val_loss: 0.0395 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "166/166 [==============================] - 0s - loss: 0.0300 - acc: 0.0120 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "166/166 [==============================] - 0s - loss: 0.0306 - acc: 0.0120 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "166/166 [==============================] - 0s - loss: 0.0284 - acc: 0.0120 - val_loss: 0.0405 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "166/166 [==============================] - 0s - loss: 0.0297 - acc: 0.0120 - val_loss: 0.0408 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "166/166 [==============================] - 0s - loss: 0.0275 - acc: 0.0120 - val_loss: 0.0411 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "166/166 [==============================] - 0s - loss: 0.0282 - acc: 0.0120 - val_loss: 0.0411 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "166/166 [==============================] - 0s - loss: 0.0295 - acc: 0.0120 - val_loss: 0.0413 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "166/166 [==============================] - 0s - loss: 0.0299 - acc: 0.0060 - val_loss: 0.0413 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "166/166 [==============================] - 0s - loss: 0.0295 - acc: 0.0120 - val_loss: 0.0410 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "166/166 [==============================] - 0s - loss: 0.0290 - acc: 0.0120 - val_loss: 0.0410 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "166/166 [==============================] - 0s - loss: 0.0294 - acc: 0.0120 - val_loss: 0.0409 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "166/166 [==============================] - 0s - loss: 0.0277 - acc: 0.0120 - val_loss: 0.0410 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "166/166 [==============================] - 0s - loss: 0.0298 - acc: 0.0060 - val_loss: 0.0410 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "166/166 [==============================] - 0s - loss: 0.0303 - acc: 0.0060 - val_loss: 0.0408 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "166/166 [==============================] - 0s - loss: 0.0310 - acc: 0.0120 - val_loss: 0.0406 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "166/166 [==============================] - 0s - loss: 0.0288 - acc: 0.0120 - val_loss: 0.0405 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "166/166 [==============================] - 0s - loss: 0.0283 - acc: 0.0120 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "166/166 [==============================] - 0s - loss: 0.0292 - acc: 0.0120 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.39485569603471871, 0.32704292830214443, 0.27848504501653004, 0.23795656734202281, 0.18335557217339435, 0.15091865536678267, 0.11867003331342375, 0.094035899064627032, 0.075133397995707507, 0.061473853931010486, 0.045391625982810213, 0.03455666847616793, 0.032478994835751604, 0.02929751920592354, 0.028343662282968141, 0.032455610403095386, 0.029969065730650741, 0.033799685964203741, 0.032199344676302137, 0.029409403071166521, 0.030921634376408106, 0.030337728739502919, 0.029199238597269517, 0.02920511557933796, 0.02753094276301114, 0.030043027734002435, 0.030535749395389156, 0.028766937576324106, 0.030324436881276499, 0.028886653078966831, 0.029890683064439212, 0.032082015805574786, 0.030045387234134847, 0.030636421842388361, 0.028379180165658515, 0.029727061582078416, 0.027504267417881863, 0.028165678123393691, 0.029490403404616446, 0.029900560851197643, 0.029528712113219571, 0.028976968545691078, 0.029407895540437066, 0.027740868055317777, 0.029835419033665257, 0.03031336718115462, 0.03095177478011114, 0.028832301923848062, 0.028252553755798972, 0.029203493866216707], 'acc': [0.0060240962508931219, 0.0060240962508931219, 0.0060240962508931219, 0.006024096385542169, 0.006024096385542169, 0.0060240962508931219, 0.0060240962508931219, 0.0060240962508931219, 0.0060240962508931219, 0.0060240962508931219, 0.0060240962508931219, 0.012048192501786244, 0.012048192636435291, 0.012048192636435291, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.012048192636435291, 0.012048192636435291, 0.012048192636435291, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.0060240962508931219, 0.012048192501786244, 0.012048192501786244, 0.012048192636435291, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.006024096385542169, 0.012048192501786244, 0.012048192501786244, 0.012048192636435291, 0.012048192501786244, 0.0060240962508931219, 0.0060240962508931219, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244, 0.012048192501786244], 'val_loss': [0.29833364486694336, 0.24878937005996704, 0.20443691313266754, 0.16593648493289948, 0.13313411176204681, 0.10546645522117615, 0.082598291337490082, 0.064678549766540527, 0.051870234310626984, 0.043570294976234436, 0.038999564945697784, 0.037570234388113022, 0.038265366107225418, 0.04017917811870575, 0.042186271399259567, 0.043914142996072769, 0.044657066464424133, 0.044717069715261459, 0.044119514524936676, 0.043125905096530914, 0.04223284125328064, 0.041401460766792297, 0.040721885859966278, 0.040276546031236649, 0.039884008467197418, 0.039486147463321686, 0.039194084703922272, 0.039049483835697174, 0.038955498486757278, 0.039056047797203064, 0.03927379846572876, 0.039529647678136826, 0.03965526819229126, 0.039941269904375076, 0.040475122630596161, 0.040803126990795135, 0.041078492999076843, 0.041128046810626984, 0.041251718997955322, 0.041252918541431427, 0.040994901210069656, 0.040968280285596848, 0.040943145751953125, 0.041006464511156082, 0.041014932096004486, 0.040804166346788406, 0.040608674287796021, 0.040458697825670242, 0.040298577398061752, 0.040200617164373398]}\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Create model\n",
    "########################################################\n",
    "\n",
    "## The LSTM network expects the input data (X) to be provided with a specific\n",
    "# array structure in the form of: [samples, time steps, features]\n",
    "# Define the network\n",
    "\n",
    "modelFit = Sequential()\n",
    "modelFit.add(LSTM(10,\n",
    "                  #activation = 'sigmoid',\n",
    "                  input_shape = (1, number_of_features)))\n",
    "modelFit.add(Dropout(.05))\n",
    "#modelFit.add(Dense(10, activation = 'relu'))\n",
    "modelFit.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "# Before training the model, configure the learning process via the compile method\n",
    "# Default settings of the optimization algorithms: https://keras.io/optimizers/\n",
    "# sgd = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# adagrad = optimizers.Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "adam = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "#sgd = optimizers.SGD(lr = 0.001, momentum = 0.0, decay = 0.0, nesterov = False)\n",
    "modelFit.compile(optimizer = adam,\n",
    "                 loss = 'mean_squared_error',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "print(modelFit.summary())\n",
    "\n",
    "# Train the model\n",
    "modelEstimate = modelFit.fit(trainX, trainY,\n",
    "                             epochs = 50,\n",
    "                             batch_size = 50,\n",
    "                             verbose = 1,\n",
    "                             validation_data = (validateX, validateY))\n",
    "\n",
    "# make predictions\n",
    "trainPredict = modelFit.predict(trainX)\n",
    "validatePredict = modelFit.predict(validateX)\n",
    "\n",
    "# print the training accuracy and validation loss at each epoch\n",
    "# print the number of models of the network\n",
    "print(modelEstimate.history)\n",
    "print(len(modelFit.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f351fc1b7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################\n",
    "# Accuracy evaluation of results\n",
    "########################################################\n",
    "\n",
    "# Invert the scaling\n",
    "df_train = np.column_stack((trainPredict, train[:, 1:]))\n",
    "trainPredict2 = scaler.inverse_transform(df_train)\n",
    "\n",
    "df_validate = np.column_stack((validatePredict, validate[:, 1:]))\n",
    "validatePredict2 = scaler.inverse_transform(df_validate)\n",
    "\n",
    "# Plot the errors of the epochs and MSE\n",
    "plt.plot(modelEstimate.history['loss'])\n",
    "plt.plot(modelEstimate.history['val_loss'])\n",
    "#  plt.plot(modelEstimate.history['val_acc'])\n",
    "plt.title('Model Error History')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training Error', 'Validation Error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Output final results\n",
    "########################################################\n",
    "\n",
    "# Combine the final datasest - merge the training and validation datasets and rename columns\n",
    "combined_dataframe = pd.concat([pd.DataFrame(trainPredict2), pd.DataFrame(validatePredict2)])\n",
    "combined_dataframe.index = range(len(combined_dataframe))\n",
    "\n",
    "# Add columns names to the data frame with the forecasts\n",
    "names_list = list(inputDataFrame)[1:]\n",
    "names_list[0] = 'lstm_forecast_brent_price'\n",
    "\n",
    "combined_dataframe.columns = names_list\n",
    "\n",
    "actual_value_target = pd.DataFrame(dataframe[:, 0])\n",
    "actual_value_target.columns = ['actual_brent_price']\n",
    "\n",
    "# Output the forecasts. Create the dataframe and write it to a CSV file\n",
    "final_forecast_file = pd.concat([actual_value_target, combined_dataframe], axis = 1)\n",
    "final_forecast_file.to_csv(dataLocation + inputFile + \"_withForecasts\" + \".csv\", sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable              Type            Data/Info\n",
      "-----------------------------------------------\n",
      "Dense                 type            <class 'keras.layers.core.Dense'>\n",
      "Dropout               type            <class 'keras.layers.core.Dropout'>\n",
      "K                     module          <module 'keras.backend' f<...>ras/backend/__init__.py'>\n",
      "LSTM                  type            <class 'keras.layers.recurrent.LSTM'>\n",
      "MinMaxScaler          type            <class 'sklearn.preprocessing.data.MinMaxScaler'>\n",
      "Sequential            type            <class 'keras.models.Sequential'>\n",
      "actual_value_target   DataFrame           actual_brent_price\\n0<...>n\\n[207 rows x 1 columns]\n",
      "brentPriceArr         ndarray         207x28: 5796 elems, type `object`, 46368 bytes\n",
      "brentPriceDf          DataFrame                 date    target <...>\\n[207 rows x 28 columns]\n",
      "combined_dataframe    DataFrame            lstm_forecast_brent_<...>\\n[207 rows x 27 columns]\n",
      "cwd                   str             /home/valentint/EAA_Analytics/Personal/VT\n",
      "dataLocation          str             /home/valentint/EAA_Analytics/Personal/VT/\n",
      "dataframe             ndarray         207x27: 5589 elems, type `object`, 44712 bytes\n",
      "dataframe_length      int             166\n",
      "dataset               ndarray         207x27: 5589 elems, type `float64`, 44712 bytes\n",
      "datasets              module          <module 'sklearn.datasets<...>rn/datasets/__init__.py'>\n",
      "df_train              ndarray         166x27: 4482 elems, type `float64`, 35856 bytes\n",
      "df_validate           ndarray         41x27: 1107 elems, type `float64`, 8856 bytes\n",
      "final_forecast_file   DataFrame           actual_brent_price  l<...>\\n[207 rows x 28 columns]\n",
      "inputData             ndarray         207x28: 5796 elems, type `object`, 46368 bytes\n",
      "inputDataFrame        DataFrame                 date    target <...>\\n[207 rows x 28 columns]\n",
      "inputFile             str             file_for_lstm_model_logdiff\n",
      "mean_squared_error    function        <function mean_squared_error at 0x7f3579bf2510>\n",
      "modelEstimate         History         <keras.callbacks.History <...>object at 0x7f35371745c0>\n",
      "modelFit              Sequential      <keras.models.Sequential <...>object at 0x7f353740c080>\n",
      "names_list            list            n=27\n",
      "np                    module          <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n",
      "number_of_features    int             26\n",
      "numpy                 module          <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n",
      "optimizers            module          <module 'keras.optimizers<...>ges/keras/optimizers.py'>\n",
      "os                    module          <module 'os' from '/usr/lib/python3.5/os.py'>\n",
      "pd                    module          <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n",
      "plt                   module          <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "randSeed              int             7896\n",
      "scaler                MinMaxScaler    MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "sgd                   SGD             <keras.optimizers.SGD object at 0x7f35372fd9e8>\n",
      "train                 ndarray         166x27: 4482 elems, type `float64`, 35856 bytes\n",
      "trainPredict          ndarray         166x1: 166 elems, type `float32`, 664 bytes\n",
      "trainPredict2         ndarray         166x27: 4482 elems, type `float64`, 35856 bytes\n",
      "trainSize             int             166\n",
      "trainX                ndarray         166x1x26: 4316 elems, type `float64`, 34528 bytes\n",
      "trainY                ndarray         166x1: 166 elems, type `float64`, 1328 bytes\n",
      "validate              ndarray         41x27: 1107 elems, type `float64`, 8856 bytes\n",
      "validatePredict       ndarray         41x1: 41 elems, type `float32`, 164 bytes\n",
      "validatePredict2      ndarray         41x27: 1107 elems, type `float64`, 8856 bytes\n",
      "validateX             ndarray         41x1x26: 1066 elems, type `float64`, 8528 bytes\n",
      "validateY             ndarray         41x1: 41 elems, type `float64`, 328 bytes\n",
      "yVarColumns           list            n=1\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
